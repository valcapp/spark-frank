{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType\n",
    "import pyspark.sql.functions as sf\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  setup\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"DegreesOfSeparation\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "spark = SparkSession.builder.appName('HerosBreadthSearch').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from enum import Enum\n",
    "# from functools import total_ordering\n",
    "\n",
    "# @total_ordering\n",
    "# class ProcessStatus(Enum):\n",
    "#     NOT_PROCESSED: str = 0\n",
    "#     PROCESSING: str = 1\n",
    "#     PROCESSED: str = 2\n",
    "#     def __eq__(self, other) -> bool:\n",
    "#         if isinstance(other, self.__class__):\n",
    "#             return self.value == other.value\n",
    "#         else:\n",
    "#             return NotImplemented\n",
    "#     def __lt__(self, other) -> bool:\n",
    "#         if isinstance(other, self.__class__):\n",
    "#             return self.value < other.value\n",
    "#         else:\n",
    "#             return NotImplemented\n",
    "\n",
    "# ProcessStatus.PROCESSED > ProcessStatus.NOT_PROCESSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- connections: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n",
      "+----+--------------------+\n",
      "|  id|         connections|\n",
      "+----+--------------------+\n",
      "|5988|[748, 1722, 3752,...|\n",
      "|5989|[4080, 4264, 4446...|\n",
      "|5982|[217, 595, 1194, ...|\n",
      "|5983|[1165, 3836, 4361...|\n",
      "|5980|[2731, 3712, 1587...|\n",
      "|5981|[3569, 5353, 4087...|\n",
      "|5986|[2658, 3712, 2650...|\n",
      "|5987|[2614, 5716, 1765...|\n",
      "|5984|[590, 4898, 745, ...|\n",
      "|5985|[3233, 2254, 212,...|\n",
      "|6294|[4898, 1127, 3220...|\n",
      "| 270|[2658, 3003, 3805...|\n",
      "| 271|[4935, 5716, 4309...|\n",
      "| 272|[2717, 4363, 4088...|\n",
      "| 273|[1165, 5013, 5110...|\n",
      "| 274|[3920, 5310, 4024...|\n",
      "| 275|[4366, 3373, 1587...|\n",
      "| 276|[2277, 5251, 4806...|\n",
      "| 277|[1068, 3495, 6194...|\n",
      "| 278|[1145, 667, 2650,...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "heroSeprationSchema = StructType([\n",
    "    StructField('id', IntegerType(), False),\n",
    "    StructField('connections', ArrayType(IntegerType(), True), False),\n",
    "])\n",
    "\n",
    "def hero_connections_parse_line(line: str) -> Row:\n",
    "    fields = line.split()\n",
    "    return Row(\n",
    "        id = int(fields[0]),\n",
    "        connections = [int(connection) for connection in fields[1:]],\n",
    "        # distance = 9999,\n",
    "        # processStatus = 0\n",
    "    )\n",
    "\n",
    "def loadStartingDF() -> DataFrame:\n",
    "    inputFile = sc.textFile(f\"file:///{os.path.abspath('')}/../data/Marvel-Graph.txt\")\n",
    "    rdd = inputFile.map(hero_connections_parse_line)\n",
    "    return spark.createDataFrame(rdd, schema=heroSeprationSchema)\n",
    "\n",
    "df = loadStartingDF()\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of ids\n",
    "# df.select(sf.collect_list('id')).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connections_by_hero(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Makes id unique, connections will be set of all connections with same id from original df\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): original df, where id's are not unique\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: where id are unique, connections are set with all connection without repetition\n",
    "        \"\"\"\n",
    "    return \\\n",
    "        df.select('id', sf.explode('connections').alias('connections'))\\\n",
    "        .groupBy('id').agg(sf.collect_set('connections').alias('connections'))\n",
    "        # this would work as well\n",
    "        # df.withColumn('connections', sf.explode('connections'))\\\n",
    "        # .groupBy('id').agg(sf.collect_set('connections').alias('connections'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BFS iteration # 1\n",
      "1741 connections were inspected.\n",
      "Running BFS iteration # 2\n",
      "214129 connections were inspected.\n",
      "Hit the target character! From 1 different direction(s).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def firstIterationDf(startDf: DataFrame, startId: int) -> DataFrame:\n",
    "    # make id unique and connections complete\n",
    "    df = connections_by_hero(startDf)\n",
    "    # mark first row to process\n",
    "    return df.withColumn(\n",
    "        'processStatus',\n",
    "        sf.when(df.id == startId, 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "def countTargetHits(df: DataFrame, targetId: int) -> int:\n",
    "    # how many heros have the targetId among their connections\n",
    "    hit_times = df.where(sf.array_contains(df.connections, targetId)).count()\n",
    "    print(f\"{df.select(sf.explode('connections')).count()} connections were inspected.\")\n",
    "    return hit_times\n",
    "\n",
    "def updateProcessStatus(df: DataFrame) -> DataFrame:\n",
    "    # get ids of next heros to process\n",
    "    isProcessing = df.processStatus == 1\n",
    "    nextIds = df.where(isProcessing)\\\n",
    "        .select(sf.explode('connections').alias('id'))\\\n",
    "        .join(df.select('id', 'processStatus'), on='id')\\\n",
    "        .where(sf.col('processStatus')==0)\\\n",
    "        .select(sf.collect_set('id')).first()[0]\n",
    "    isToProcessNext = df.id.isin(nextIds)\n",
    "    # mark current processing as processed, next to be processed as processing\n",
    "    return \\\n",
    "        df.select(\n",
    "            'id', 'connections',\n",
    "            sf.when(isProcessing, 2)\\\n",
    "                .when(isToProcessNext, 1)\\\n",
    "                .otherwise(df.processStatus)\\\n",
    "                .alias('processStatus')\n",
    "        )\n",
    "\n",
    "def degreeSeparation(startDf: DataFrame, startId: int, targetId: int) -> int:\n",
    "    df = firstIterationDf(startDf, startId)\n",
    "\n",
    "    for distance in range (1, 11):\n",
    "        print(f\"Running BFS iteration # {distance}\")\n",
    "        # process layer (rows with process status 1)\n",
    "        hit_times = countTargetHits(df.where(df.processStatus == 1), targetId)\n",
    "        if hit_times:\n",
    "            print(f\"Hit the target character! From {hit_times} different direction(s).\")\n",
    "            break\n",
    "        df = updateProcessStatus(df)\n",
    "    \n",
    "    return distance\n",
    "\n",
    "degreeSeparation(loadStartingDF(), 5306, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a691c954e455b59e9eca8d8310234eae5f9ba2125f4627db7e62764b4671cef3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('spark-_Kvxx3OO': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
